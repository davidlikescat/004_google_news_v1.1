import requests
from bs4 import BeautifulSoup
import time
import logging
from urllib.parse import urljoin, urlparse
import re
from config import Config

class ArticleCrawler:
    """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.config = Config()
        self.logger = logging.getLogger(__name__)
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.config.USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_attempts': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'fallback_used': 0
        }
        
        # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì„ íƒì (ì‚¬ì´íŠ¸ ìµœì í™”)
        self.content_selectors = {
            'aitimes.com': ['.article-content', '.news-content', 'article'],
            'techcrunch.com': ['.entry-content', '.article-content'],
            'zdnet.co.kr': ['.view_con', '.article_view', '.news-content'],
            'chosun.com': ['.news_text', '.article_view', '.news-content'],
            'dt.co.kr': ['.article-content', '.view-con', '.news-content'],
            'etnews.com': ['.article_txt', '.news-content', '.article-content'],
            'hankyung.com': ['.article-content', '.news-content'],
            'mk.co.kr': ['.news_detail_text', '.article-content']
        }
        
        # ì œê±°í•  ìš”ì†Œë“¤
        self.remove_selectors = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            '.advertisement', '.ad', '.ads', '.social-share',
            '.related-articles', '.comment', '.navigation',
            '.breadcrumb', '.tag', '.share', '.print'
        ]
    
    def crawl_articles(self, articles):
        """ê¸°ì‚¬ ëª©ë¡ í¬ë¡¤ë§"""
        self.logger.info(f"ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘: {len(articles)}ê°œ")
        
        crawled_articles = []
        self.stats['total_attempts'] = len(articles)
        
        for i, article in enumerate(articles, 1):
            try:
                print(f"ğŸ“„ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘... ({i}/{len(articles)}) - {article['title'][:50]}...")
                
                content, images = self._extract_content(article['link'])
                
                # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
                article['content'] = content
                article['images'] = images
                article['content_length'] = len(content)
                article['crawl_status'] = 'success' if content else 'failed'
                article['crawled_at'] = time.time()
                
                crawled_articles.append(article)
                
                if content:
                    self.stats['successful_crawls'] += 1
                    self.logger.info(f"í¬ë¡¤ë§ ì„±ê³µ: {article['title'][:50]}... ({len(content)}ì)")
                else:
                    self.stats['failed_crawls'] += 1
                    self.logger.warning(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {article['title'][:50]}...")
                
                # ìš”ì²­ ê°„ê²©
                time.sleep(self.config.REQUEST_DELAY)
                
            except Exception as e:
                self.stats['failed_crawls'] += 1
                self.logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ - {article['title']}: {e}")
                
                # ì‹¤íŒ¨ì‹œ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
                article['content'] = article.get('summary', '')
                article['images'] = []
                article['content_length'] = len(article['content'])
                article['crawl_status'] = 'error'
                article['crawled_at'] = time.time()
                
                crawled_articles.append(article)
        
        self._print_statistics()
        return crawled_articles
    
    def _extract_content(self, url):
        """ì›¹í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # HTTP ìš”ì²­
            response = self.session.get(url, timeout=self.config.TIMEOUT)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ì„¤ì •
            if response.encoding.lower() in ['iso-8859-1', 'ascii']:
                response.encoding = 'utf-8'
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_main_content(soup, url)
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = self._extract_images(soup, url)
            
            return content, images
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"HTTP ìš”ì²­ ì‹¤íŒ¨ ({url}): {e}")
            return "", []
        except Exception as e:
            self.logger.error(f"ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return "", []
    
    def _extract_main_content(self, soup, url):
        """ë©”ì¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        # ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ì„ íƒì ì‚¬ìš©
        domain = urlparse(url).netloc.lower()
        
        selectors = []
        for site_domain, site_selectors in self.content_selectors.items():
            if site_domain in domain:
                selectors = site_selectors
                break
        
        # ê¸°ë³¸ ì„ íƒì ì¶”ê°€
        if not selectors:
            selectors = [
                'article', '.article-content', '.news-content', '.post-content',
                '.entry-content', '#content', '.article-body', '.news-body',
                '.content', '.main-content', '.article', '.post'
            ]
        
        # ì„ íƒìë³„ë¡œ ì»¨í…ì¸  ì¶”ì¶œ ì‹œë„
        for selector in selectors:
            content_element = soup.select_one(selector)
            if content_element:
                content = self._clean_content(content_element)
                if len(content) > 200:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    return content
        
        # í´ë°±: p íƒœê·¸ë“¤ ìˆ˜ì§‘
        paragraphs = soup.find_all('p')
        if paragraphs:
            self.stats['fallback_used'] += 1
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            return self._clean_text(content)
        
        # ìµœí›„ì˜ ìˆ˜ë‹¨: body í…ìŠ¤íŠ¸
        body = soup.find('body')
        if body:
            return self._clean_text(body.get_text()[:1000])
        
        return ""
    
    def _clean_content(self, element):
        """ì»¨í…ì¸  ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for selector in self.remove_selectors:
            for tag in element.select(selector):
                tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = element.get_text(separator=' ', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        return self._clean_text(text)
    
    def _clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = text.replace('\u200b', '')  # Zero width space
        text = text.replace('\ufeff', '')  # BOM
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        import html
        text = html.unescape(text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def _extract_images(self, soup, base_url):
        """ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        images = []
        
        # img íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                absolute_url = urljoin(base_url, src)
                
                # ê¸°ë³¸ì ì¸ ì´ë¯¸ì§€ í•„í„°ë§
                if self._is_valid_image(absolute_url):
                    images.append({
                        'url': absolute_url,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
        
        return images[:5]  # ìµœëŒ€ 5ê°œë§Œ
    
    def _is_valid_image(self, url):
        """ìœ íš¨í•œ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        # ê¸°ë³¸ í™•ì¥ì ì²´í¬
        valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        url_lower = url.lower()
        
        if any(ext in url_lower for ext in valid_extensions):
            return True
        
        # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ë‚˜ ì•„ì´ì½˜ ì œì™¸
        exclude_keywords = ['icon', 'logo', 'avatar', 'thumbnail', '1x1', 'pixel']
        if any(keyword in url_lower for keyword in exclude_keywords):
            return False
        
        return True
    
    def _print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        success_rate = (self.stats['successful_crawls'] / self.stats['total_attempts'] * 100) if self.stats['total_attempts'] > 0 else 0
        
        print(f"\nğŸ“Š í¬ë¡¤ë§ í†µê³„:")
        print(f"  â€¢ ì‹œë„: {self.stats['total_attempts']}ê°œ")
        print(f"  â€¢ ì„±ê³µ: {self.stats['successful_crawls']}ê°œ ({success_rate:.1f}%)")
        print(f"  â€¢ ì‹¤íŒ¨: {self.stats['failed_crawls']}ê°œ")
        print(f"  â€¢ í´ë°± ì‚¬ìš©: {self.stats['fallback_used']}ê°œ")
    
    def test_single_article(self, url):
        """ë‹¨ì¼ ê¸°ì‚¬ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ§ª ë‹¨ì¼ ê¸°ì‚¬ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸: {url}")
        
        try:
            content, images = self._extract_content(url)
            
            print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ")
            print(f"  â€¢ ë³¸ë¬¸ ê¸¸ì´: {len(content)}ì")
            print(f"  â€¢ ì´ë¯¸ì§€ ìˆ˜: {len(images)}ê°œ")
            print(f"  â€¢ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
            
            if images:
                print(f"  â€¢ ì²« ë²ˆì§¸ ì´ë¯¸ì§€: {images[0]['url']}")
            
            return content, images
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return "", []
    
    def get_statistics(self):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return self.stats.copy()
    
    def add_custom_selector(self, domain, selectors):
        """ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í…€ ì„ íƒì ì¶”ê°€"""
        self.content_selectors[domain] = selectors
        self.logger.info(f"ì»¤ìŠ¤í…€ ì„ íƒì ì¶”ê°€: {domain} -> {selectors}")
    
    def get_content_preview(self, article):
        """ê¸°ì‚¬ ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°"""
        content = article.get('content', '')
        if len(content) > 300:
            return content[:300] + "..."
        return content