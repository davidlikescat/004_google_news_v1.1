# google_news_collector.py - Google News ìµœì‹  AI ê¸°ì‚¬ ìˆ˜ì§‘
import requests
from datetime import datetime, timedelta
import time
import re
from urllib.parse import quote
from bs4 import BeautifulSoup
import feedparser
import logging

logger = logging.getLogger(__name__)

class GoogleNewsCollector:
    """Google Newsì—ì„œ ìµœì‹  AI ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, max_articles=5):
        self.max_articles = max_articles
        self.base_url = "https://news.google.com/rss/search"
        self.session = requests.Session()
        
        # User-Agent ì„¤ì • (Google News ì ‘ê·¼ìš©)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml,application/xml,text/xml,*/*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })
    
    def build_search_query(self, keywords):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        # í•µì‹¬ AI í‚¤ì›Œë“œë§Œ ì‚¬ìš© (ìµœì‹ ì„± ìš°ì„ )
        core_keywords = [
            'ì¸ê³µì§€ëŠ¥', 'AI', 'ìƒì„±í˜•AI', 'ChatGPT', 'LLM', 
            'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'GPT', 'ììœ¨ì£¼í–‰', 'AIë°˜ë„ì²´'
        ]
        
        # OR ì—°ì‚°ìë¡œ í‚¤ì›Œë“œ ì—°ê²°
        query_parts = []
        for keyword in core_keywords[:5]:  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            query_parts.append(f'"{keyword}"')
        
        search_query = ' OR '.join(query_parts)
        
        # í•œêµ­ ê¸°ì‚¬ ìš°ì„ , ìµœê·¼ 24ì‹œê°„ ì´ë‚´
        search_query += ' when:1d'  # ìµœê·¼ 1ì¼ ì´ë‚´
        
        return search_query
    
    def collect_latest_news(self, keywords):
        """Google Newsì—ì„œ ìµœì‹  AI ë‰´ìŠ¤ ìˆ˜ì§‘"""
        print(f"ğŸ” Google Newsì—ì„œ ìµœì‹  AI ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        search_query = self.build_search_query(keywords)
        encoded_query = quote(search_query)
        
        # Google News RSS URL êµ¬ì„±
        rss_url = f"{self.base_url}?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        print(f"ğŸ“¡ ê²€ìƒ‰ URL: {rss_url}")
        
        try:
            # RSS í”¼ë“œ íŒŒì‹±
            response = self.session.get(rss_url, timeout=30)
            response.raise_for_status()
            
            # feedparserë¡œ RSS íŒŒì‹±
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                print("âš ï¸ Google Newsì—ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            articles = []
            processed_count = 0
            
            print(f"ğŸ“° {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬, ìµœëŒ€ {self.max_articles}ê°œ ìˆ˜ì§‘...")
            
            for entry in feed.entries:
                if processed_count >= self.max_articles:
                    break
                
                try:
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    article = self.extract_article_info(entry)
                    
                    if article and self.is_ai_related(article, keywords):
                        articles.append(article)
                        processed_count += 1
                        print(f"âœ… [{processed_count}] {article['title'][:50]}...")
                    
                    # ìš”ì²­ ê°„ ë”œë ˆì´
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"ğŸ¯ ì´ {len(articles)}ê°œ AI ê´€ë ¨ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles
            
        except Exception as e:
            logger.error(f"Google News ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(f"âŒ Google News ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_article_info(self, entry):
        """RSS entryì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            # Google News URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
            original_url = self.extract_original_url(entry.link)
            
            # ë°œí–‰ ì‹œê°„ íŒŒì‹±
            published_time = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                published_time = datetime(*entry.published_parsed[:6])
            else:
                published_time = datetime.now()
            
            # ì¶œì²˜ ì¶”ì¶œ
            source = "Unknown"
            if hasattr(entry, 'source') and entry.source:
                source = entry.source.get('title', 'Unknown')
            
            article = {
                'title': entry.title,
                'url': original_url or entry.link,
                'published': published_time,
                'source': source,
                'summary': getattr(entry, 'summary', '')[:200],
                'content': '',  # ë‚˜ì¤‘ì— í¬ë¡¤ë§ìœ¼ë¡œ ì±„ì›€
                'keywords': [],
                'category': 'ìµœì‹  AI ë‰´ìŠ¤'
            }
            
            return article
            
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_original_url(self, google_news_url):
        """Google News URLì—ì„œ ì›ë³¸ ê¸°ì‚¬ URL ì¶”ì¶œ"""
        try:
            # Google News ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ì²˜ë¦¬
            if 'news.google.com' in google_news_url:
                # URL íŒŒë¼ë¯¸í„°ì—ì„œ ì‹¤ì œ URL ì°¾ê¸°
                if 'url=' in google_news_url:
                    import urllib.parse as urlparse
                    parsed = urlparse.urlparse(google_news_url)
                    params = urlparse.parse_qs(parsed.query)
                    if 'url' in params:
                        return params['url'][0]
                
                # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹¤ì œ URL ì¶”ì¶œ ì‹œë„
                response = self.session.head(google_news_url, allow_redirects=True, timeout=10)
                return response.url
            
            return google_news_url
            
        except Exception as e:
            logger.warning(f"ì›ë³¸ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return google_news_url
    
    def is_ai_related(self, article, keywords):
        """ê¸°ì‚¬ê°€ AI ê´€ë ¨ì¸ì§€ í™•ì¸"""
        text_to_check = f"{article['title']} {article['summary']}".lower()
        
        # í•µì‹¬ AI í‚¤ì›Œë“œ ì²´í¬
        ai_keywords = [
            'ì¸ê³µì§€ëŠ¥', 'ai', 'ìƒì„±í˜•ai', 'chatgpt', 'gpt', 'llm',
            'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§', 'ììœ¨ì£¼í–‰', 'aië°˜ë„ì²´',
            'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'ì‚¼ì„±ai', 'í´ë¡œë°”'
        ]
        
        for keyword in ai_keywords:
            if keyword.lower() in text_to_check:
                return True
        
        return False
    
    def filter_recent_articles(self, articles, hours=24):
        """ìµœê·¼ ì‹œê°„ ë‚´ ê¸°ì‚¬ë§Œ í•„í„°ë§"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_articles = []
        
        for article in articles:
            if article['published'] >= cutoff_time:
                recent_articles.append(article)
        
        # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
        recent_articles.sort(key=lambda x: x['published'], reverse=True)
        
        print(f"ğŸ“… ìµœê·¼ {hours}ì‹œê°„ ì´ë‚´ ê¸°ì‚¬: {len(recent_articles)}ê°œ")
        return recent_articles[:self.max_articles]

def test_google_news_collector():
    """Google News ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Google News ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    from config import Config
    
    collector = GoogleNewsCollector(max_articles=3)
    articles = collector.collect_latest_news(Config.AI_KEYWORDS)
    
    if articles:
        print(f"\nâœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ:")
        for i, article in enumerate(articles, 1):
            print(f"{i}. {article['title']}")
            print(f"   ì¶œì²˜: {article['source']}")
            print(f"   ë°œí–‰: {article['published'].strftime('%Y-%m-%d %H:%M')}")
            print(f"   URL: {article['url']}")
            print()
    else:
        print("âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    test_google_news_collector()